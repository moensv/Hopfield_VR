\section{Exercise 3: Random asymmetry}

\begin{itshape}
\small
To investigate the robustness of pattern retrieval against asymmetry in the weight matrix, consider now networks where for each pairs of nodes $\l(i, j\r)$, the directed connection from node $i$ to node $j$ is cut with probability $p_{cut}$. This will introduce asymmetry in the Hebbian weight matrix Eq. 1.

Set $c=0.1$ and $N = 200$. As in Ex. 2 calculate and plot the mean $\alpha_{N,max}$ for varying $p_{cut}$ with error bars (at least 10 repetitions). At which $p_{cut}$ does the maximal load drop below $50\%$ of the value estimated in Ex. 2? State the confidence interval.

Note: Bare in mind that the convergence assertion of Question 1.1 does not necessarily hold for $p_{cut} > 0$.
\end{itshape}

\paragraph*{}



\begin{figure}[H]
  \vspace{-20pt}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{dat/ex3-mean_max_load-N200-Q50-C95}
  \end{center}
  \vspace{-20pt}
  \caption{Exercise 3: Maximal load $\alpha_{N,max}$ averaged over 10 repetitions for varying $p_{cut}$. The data is normalized to the values of $\alpha_{N,max}=0.1278$ found in exercise 2. }
  \label{fig:exercise3}
  \vspace{-10pt}
\end{figure}

The decrease in the mean maximal load seems to be nearly linear with its error in figure \ref{fig:exercise3} decreasing as $p_cut$ increases. The decrease in the error  can be explained through equation \ref{eq: sequential dynamics}, where a cutting of directed weights leads to a reduction of the possible extrema of the sum in the same equation which decreases the variance of that sum and therefore the error in the possible outcome of the equation. The error was 0, as expected, when $p_{cut} =1$. At $p_{cut} = 0.6$ the maximal load drops below $50\%$ of the value estimated in exercise 2. The confidence intervals are listed in table 2.

\begin{table}[H] 
\label{tbl:exercise3_CI}
\centering 
\begin{tabular}{|l|l|l|l|l|l|l|l|} 
\hline 
\input{dat/ex3-table-N200-Q50-C95.tex}	
\hline
\end{tabular}
\caption{Table giving the confidence intervals for all data points in figure \ref{fig:exercise3}.}
\end{table} 
 
